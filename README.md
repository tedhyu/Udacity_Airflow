# Udacity-Airflow Project
Project 5 for Udacity- Data Engineering nanodegree

<hr/>

## How to install and get it up and running

Requirements:  Python (os, pyspark.sql, datetime, configparser)

1)  Copy the "dags" and "plugins" directories under the airflow directory in Udacity container
2)  In Udacity workspace prompt, enter "/opt/airflow/start.sh"
3)  Check dags by typing "airflow list_dags"
4)  Click "Access Airflow" button
5)  In the GUI, click Admin -> Connections:  Conn Id - aws_credentials, Conn Type - Amazon Web Services, Login (Access key ID), Password (Secret Access Key)
6)  In the GUI, click Admin -> Connections:  Conn Id - redshift, Conn Type - Postgres, Host (Cluster Address), Schema (Cluster Name), Login (Login Name), Password (Login Password), Port - 5439
7)  Under udac_example_dag, turn it "On".
8)  Press Play button.


## Introduction
Airflow dag that copies data from song and log JSON files from Udacity S3 bucket and creates seven tables on Redshift:  staging_events, staging_songs, songplays, songs, artists, titles, and time.  The dag is set to retry three times with a five minute delay on failure.  The tables are checked for data integrity.

## Dataset
CSong Dataset:
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

Log Dataset:
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Tables
Staging Tables:
staging_events - data from bucket:  s3://udacity-dend/log-data/      Use schema from:  s3://udacity-dend/log_json_path.json
artist,	auth, firstname, gender, iteminsession,	lastname, length, level, location, method, page, registration, sessionid, song,	status,	ts, useragent, userid

staging_songs - data from bucket:  s3://udacity-dend/song_data/     Use "auto" schema
num_songs, artist_id, artist_name, artist_latitude, artist_longitude, artist_location, song_id, title, duration, year

Fact Table:
songplays - records in log data associated with song plays i.e. records with page NextSong
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables:
users - users in the app
user_id, first_name, last_name, gender, level

songs - songs in music database
song_id, title, artist_id, year, duration

artists - artists in music database
artist_id, name, location, latitude, longitude

time - timestamps of records in songplays broken down into specific units
start_time, hour, day, week, month, year, weekday
